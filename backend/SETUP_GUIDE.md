# Quick Setup Guide - Comprehensive Energy Grid Pipeline

## üéØ What You're Building

A **Palantir Foundry-style ontology** for the DEG Compute-Energy Convergence challenge that:
- Fetches live UK energy grid data every 60 seconds
- Stores in Supabase with UUID primary keys and proper relations
- Enables carbon-aware compute scheduling
- Supports multi-agent orchestration
- Tracks P415 flexibility participation

## üì¶ What Was Created

### **Core Files** (3)
1. `schema_comprehensive.sql` - Database schema (22 tables, 4 views, 14 UK regions)
2. `energy_data_fetcher.py` - Fetches from 3 UK energy APIs
3. `comprehensive_pipeline.py` - Main pipeline (runs every 60s)

### **Test & Support Files** (4)
4. `test_comprehensive_pipeline.py` - Test suite
5. `COMPREHENSIVE_PIPELINE_README.md` - Full documentation
6. `SETUP_GUIDE.md` - This file
7. `requirements_pipeline.txt` - Python dependencies

## üöÄ Setup (5 Minutes)

### **Step 1: Install Dependencies**

```bash
cd backend
pip install -r requirements_pipeline.txt
```

### **Step 2: Configure Supabase**

Create `.env` file:

```bash
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your-service-role-key
```

**Important:** Use the `service_role` key (not `anon` key) from Supabase Dashboard ‚Üí Settings ‚Üí API.

### **Step 3: Initialize Database**

1. Go to Supabase Dashboard
2. Click **SQL Editor**
3. Copy **all contents** of `schema_comprehensive.sql`
4. Paste and click **Run**

You should see:
- ‚úÖ 22 tables created
- ‚úÖ 14 UK regions inserted
- ‚úÖ 4 views created
- ‚úÖ Indexes and triggers created

### **Step 4: Test the Pipeline**

```bash
python3 test_comprehensive_pipeline.py
```

Expected output:
```
‚úì Carbon intensity (national)
‚úì Carbon intensity (regional)
‚úì Generation mix (national)
‚úì Generation mix (regional)
‚úì Demand forecast
‚úì Beckn data

ALL TESTS PASSED ‚úì
```

### **Step 5: Run the Pipeline**

```bash
python3 comprehensive_pipeline.py
```

The pipeline will:
- Fetch data every 60 seconds
- Store in Supabase automatically
- Log to `comprehensive_pipeline.log` and console

## üìä Verify Data is Flowing

### **Option 1: Supabase Dashboard**

Go to **Table Editor** and check these tables have data:
- `carbon_intensity_national` - Should have ~48 rows (48-hour forecast)
- `carbon_intensity_regional` - Should have ~14 rows (14 UK regions)
- `demand_forecast_national` - Should have ~48-96 rows
- `generation_mix_national` - Should have 1 row
- `grid_zones` - Should have ~9 zones (from Beckn)
- `grid_snapshots` - Should have ~9 snapshots
- `api_logs` - Should show successful API calls

### **Option 2: SQL Query**

```sql
-- Check latest data across all sources
SELECT
    'Carbon (National)' as data_type,
    COUNT(*) as records,
    MAX(timestamp) as latest_data
FROM carbon_intensity_national
UNION ALL
SELECT
    'Carbon (Regional)',
    COUNT(*),
    MAX(timestamp)
FROM carbon_intensity_regional
UNION ALL
SELECT
    'Demand Forecast',
    COUNT(*),
    MAX(timestamp)
FROM demand_forecast_national
UNION ALL
SELECT
    'Beckn Windows',
    COUNT(*),
    MAX(snapshot_timestamp)
FROM grid_snapshots;
```

Should show recent timestamps (within last few minutes).

## üóÑÔ∏è Database Schema Overview

### **22 Tables in 5 Categories:**

#### **1. Beckn Compute Data (5 tables)**
- `grid_zones` - Geographic zones (Cambridge, Manchester, etc.)
- `compute_windows` - Time windows for compute
- `grid_snapshots` - Time-series renewable mix & carbon
- `offers` - Pricing (¬£/kWh)
- `beckn_transactions` - Transaction logs

#### **2. UK Energy Grid (8 tables)**
- `uk_regions` - 14 UK DNO regions (reference table)
- `carbon_intensity_national` - National carbon data
- `carbon_intensity_regional` - Regional carbon data
- `generation_mix_national` - National fuel mix
- `generation_mix_regional` - Regional fuel mix
- `demand_forecast_national` - Demand forecasts
- `demand_actual_national` - Actual demand
- `demand_regional` - Regional demand

#### **3. Pricing (2 tables)**
- `wholesale_prices` - Wholesale electricity prices
- `flexibility_prices` - P415 flexibility pricing

#### **4. Compute Workloads (4 tables)**
- `compute_assets` - Data centers
- `compute_workloads` - Individual jobs
- `workload_schedules` - Scheduling decisions
- `agent_negotiations` - Multi-agent negotiations

#### **5. System (3 tables)**
- `agents` - Agent registry
- `beckn_transactions` - Beckn order lifecycle
- `api_logs` - API monitoring

### **4 Views for Analytics:**
- `current_grid_status` - Real-time national status
- `regional_compute_opportunities` - Best regions for low-carbon compute
- `workload_optimization_opportunities` - Deferrable workloads
- `latest_grid_conditions` - Latest Beckn data

## üìà Example Queries

### **1. Current Grid Status**

```sql
SELECT * FROM current_grid_status;
```

Returns:
- Carbon intensity (gCO2/kWh)
- Renewable percentage
- National demand (MW)
- Grid stress score
- Wholesale price (¬£/MWh)

### **2. Find Greenest Regions**

```sql
SELECT
    region_name,
    forecast_gco2_kwh as carbon_intensity,
    intensity_index
FROM carbon_intensity_regional cir
JOIN uk_regions ur ON cir.region_id = ur.id
WHERE timestamp = (SELECT MAX(timestamp) FROM carbon_intensity_regional)
ORDER BY forecast_gco2_kwh ASC
LIMIT 5;
```

### **3. Best Compute Windows (Low Carbon + Low Cost)**

```sql
SELECT
    window_name,
    grid_area,
    renewable_mix,
    carbon_intensity,
    beckn_price_value as price_gbp_kwh,
    window_start,
    window_end
FROM latest_grid_conditions
WHERE renewable_mix >= 75
ORDER BY carbon_intensity ASC, beckn_price_value ASC;
```

## üîç Monitoring

### **Check Pipeline Health**

```bash
# Watch logs in real-time
tail -f comprehensive_pipeline.log

# Check if running
ps aux | grep comprehensive_pipeline
```

### **Check API Success Rate**

```sql
SELECT
    api_name,
    COUNT(*) as total_calls,
    COUNT(*) FILTER (WHERE status_code = 200) as successful,
    COUNT(*) FILTER (WHERE error_message IS NOT NULL) as errors,
    MAX(request_timestamp) as last_call
FROM api_logs
WHERE request_timestamp >= NOW() - INTERVAL '1 hour'
GROUP BY api_name;
```

## üêõ Troubleshooting

### **Problem: "Missing SUPABASE_URL or SUPABASE_KEY"**

**Solution:**
1. Create `.env` file in `backend/` folder
2. Add credentials from Supabase Dashboard
3. Use `service_role` key, not `anon` key

### **Problem: "Table does not exist"**

**Solution:**
1. Run `schema_comprehensive.sql` in Supabase SQL Editor
2. Verify tables exist in Supabase Table Editor

### **Problem: "No data being inserted"**

**Solution:**
1. Check API logs: `SELECT * FROM api_logs ORDER BY request_timestamp DESC LIMIT 10;`
2. Check pipeline logs: `tail -f comprehensive_pipeline.log`
3. Verify internet connectivity
4. Check if APIs are accessible (Carbon Intensity API, NESO API, Beckn API)

### **Problem: "Carbon Intensity API returns 400"**

**Solution:** This is normal for some timestamp formats. The fetcher handles it automatically by using current timestamp.

### **Problem: "NESO API returns no data"**

**Solution:** The pipeline has a synthetic fallback that generates realistic UK demand patterns. This is intentional.

## üéì Next Steps

### **For DEG Challenge:**

1. **Implement Compute Agents**
   - Register compute assets in `compute_assets` table
   - Create workloads in `compute_workloads` table
   - Use `workload_optimization_opportunities` view to find deferrable workloads

2. **Implement Orchestration Logic**
   - Query `current_grid_status` for decision-making
   - Log decisions in `workload_schedules` table
   - Track agent negotiations in `agent_negotiations` table

3. **Build Dashboard**
   - Query views for real-time data
   - Show compute-energy flows
   - Display cost savings and carbon reductions

4. **Add Beckn Order Lifecycle**
   - Use `beckn_transactions` table
   - Implement discover ‚Üí search ‚Üí select ‚Üí init ‚Üí confirm flow
   - Track order status

### **For Production:**

1. **Set up continuous running** (see [COMPREHENSIVE_PIPELINE_README.md](COMPREHENSIVE_PIPELINE_README.md) for systemd/Docker/K8s)
2. **Add alerting** (Slack/email when carbon intensity spikes)
3. **Add data retention** (archive old data after 30 days)
4. **Add backup** (Supabase handles this automatically)

## üìö Documentation

- **Full docs:** [COMPREHENSIVE_PIPELINE_README.md](COMPREHENSIVE_PIPELINE_README.md)
- **Original Beckn pipeline:** [GRID_PIPELINE_README.md](GRID_PIPELINE_README.md)
- **Carbon Intensity API:** https://carbon-intensity.github.io/api-definitions/
- **National Grid ESO:** https://data.nationalgrideso.com/
- **Beckn Protocol:** https://github.com/beckn/protocol-specifications

## üéØ Key Metrics You Can Now Track

‚úÖ **Carbon intensity** (national & regional, gCO2/kWh)
‚úÖ **Renewable energy mix** (%, by fuel type)
‚úÖ **Electricity demand** (MW, with forecasts)
‚úÖ **Grid stress** (0-1 normalized score)
‚úÖ **Wholesale prices** (¬£/MWh)
‚úÖ **Compute windows** (optimal times for workloads)
‚úÖ **Flexibility opportunities** (P415 participation)

All updated **every 60 seconds** automatically! üöÄ

---

**You now have a production-ready energy grid data platform for compute-energy convergence!**
